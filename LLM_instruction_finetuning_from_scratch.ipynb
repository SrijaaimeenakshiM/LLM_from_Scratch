{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnjK4jdehDcV28Qgx0adFB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrijaaimeenakshiM/LLM_from_Scratch/blob/main/LLM_instruction_finetuning_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBF8cu04ZmQG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/instruction-data.json\", 'r') as f:\n",
        "\n",
        "    data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCFfI-AqcEZI",
        "outputId": "b2ca0737-c1b5-40cd-f7ff-3691db3941b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1100"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g7r_wiQbw_Q",
        "outputId": "4a1a0c0a-72de-473f-c8f8-358b668ca1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''converting input into the format\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Identify the correct spelling of the following word.\n",
        "\n",
        "### Input:\n",
        "Ocassion\n",
        "\n",
        "### Response:\n",
        "The correct spelling is 'Occasion.'''\n",
        "#This is known as Alpaca format"
      ],
      "metadata": {
        "id": "A97FWAmycAtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(input):\n",
        "    formatted_input = (\n",
        "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n## Instruction:\\n{input['instruction']}\"\n",
        "    )\n",
        "\n",
        "    # Conditionally add input section\n",
        "    if input['input']:\n",
        "        formatted_input += f\"\\n\\n## Input:\\n{input['input']}\"\n",
        "\n",
        "    formatted_input += f\"\\n\\n## Response:\\n{input['output']}\"\n",
        "    return formatted_input\n"
      ],
      "metadata": {
        "id": "AqlMFOjUce-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=format_input(data[50])\n",
        "print(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7MIuDsFeOT6",
        "outputId": "35b4f966-ffbe-4783-87c7-d244c6c32313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "## Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "## Input:\n",
            "Ocassion\n",
            "\n",
            "## Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class Instruction_Dataset(Dataset):\n",
        "  def __init__(self,data,tokenizer):\n",
        "    self.data=data\n",
        "    self.encoded_input=[]\n",
        "\n",
        "    for i in data:\n",
        "      full_text=format_input(i)\n",
        "      self.encoded_input.append(tokenizer.encode(full_text))\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.encoded_input[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n"
      ],
      "metadata": {
        "id": "QG0XQq3AhmQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the train_test data\n",
        "\n",
        "train_portion=int(0.90*len(data))\n",
        "test_portion=int(0.10*len(data))\n",
        "\n",
        "print(train_portion)\n",
        "print(test_portion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W3yKXMges3g",
        "outputId": "9f0d56c9-d30d-41bf-b21f-087905734b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "990\n",
            "110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]"
      ],
      "metadata": {
        "id": "yeMboX6M9WDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We should add padding to make all the inputs are of same size\n",
        "\n",
        "def custom_collate_draft(batch,device,pad_token_id=50256):\n",
        "\n",
        "  max_len=max(len(input)+1 for input in batch)\n",
        "\n",
        "  inputs=[]\n",
        "\n",
        "  for input in batch:\n",
        "    new_input=input.copy()\n",
        "\n",
        "    new_input+=[pad_token_id]\n",
        "\n",
        "    padded=(new_input+[pad_token_id]*(max_len-len(new_input)))\n",
        "    inputs.append(torch.tensor(padded[:-1]))\n",
        "\n",
        "  input_tensor=torch.stack(inputs).to(device)\n",
        "\n",
        "  return input_tensor\n"
      ],
      "metadata": {
        "id": "uuPD_7Xnf9Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft(batch,device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDKKyCG3tNgD",
        "outputId": "e52191ed-e97c-4b24-bc9a-c7276af5d1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    device,\n",
        "    pad_token_id=50256\n",
        "\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "xC9uNeqXtu8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_draft_2(batch,device)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZWYU2Q9zNlF",
        "outputId": "e464b32b-f576-4a48-e32d-a2fbb63b8bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]], device='cuda:0')\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "     device,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None\n",
        "\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "l7eFRVEHzPvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In PyTorch, when using CrossEntropyLoss (or nn.functional.cross_entropy), any target value equal to -100 is ignored during loss computation.\n",
        "\n",
        "#-100 means: “Don't compute loss for this token.”\n",
        "\n",
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch,device)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OrS8nwi64RA",
        "outputId": "7e4c4a33-2e63-4f60-9849-c947ecaf277c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]], device='cuda:0')\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ],
      "metadata": {
        "id": "zCyywVDv68g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = Instruction_Dataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = Instruction_Dataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "7by9hdhw8v0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nyupjnX9kbM",
        "outputId": "afe823b9-92bc-4c63-e7ad-43fafa1cf406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 42]) torch.Size([8, 42])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 56]) torch.Size([8, 56])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 25]) torch.Size([8, 25])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 56]) torch.Size([8, 56])\n",
            "torch.Size([8, 56]) torch.Size([8, 56])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 44]) torch.Size([8, 44])\n",
            "torch.Size([8, 19]) torch.Size([8, 19])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 54]) torch.Size([8, 54])\n",
            "torch.Size([8, 22]) torch.Size([8, 22])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 54]) torch.Size([8, 54])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 44]) torch.Size([8, 44])\n",
            "torch.Size([8, 49]) torch.Size([8, 49])\n",
            "torch.Size([8, 44]) torch.Size([8, 44])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 27]) torch.Size([8, 27])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 23]) torch.Size([8, 23])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 56]) torch.Size([8, 56])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 24]) torch.Size([8, 24])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 42]) torch.Size([8, 42])\n",
            "torch.Size([8, 52]) torch.Size([8, 52])\n",
            "torch.Size([8, 53]) torch.Size([8, 53])\n",
            "torch.Size([8, 42]) torch.Size([8, 42])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 34]) torch.Size([8, 34])\n",
            "torch.Size([8, 46]) torch.Size([8, 46])\n",
            "torch.Size([8, 44]) torch.Size([8, 44])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 45]) torch.Size([8, 45])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 55]) torch.Size([8, 55])\n",
            "torch.Size([8, 44]) torch.Size([8, 44])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 41]) torch.Size([8, 41])\n",
            "torch.Size([8, 51]) torch.Size([8, 51])\n",
            "torch.Size([8, 43]) torch.Size([8, 43])\n",
            "torch.Size([8, 47]) torch.Size([8, 47])\n",
            "torch.Size([8, 48]) torch.Size([8, 48])\n",
            "torch.Size([8, 50]) torch.Size([8, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "XY7JUetW9qmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768\n",
        "\n"
      ],
      "metadata": {
        "id": "TrUo3fN9C4Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#architecture of LLM\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "aHnPP_D7RBQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "n51AuumtPmhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n"
      ],
      "metadata": {
        "id": "kZ1FDqNiUoCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "RqFbdBW2C9L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings,params=download_and_load_gpt2(model_size=\"355M\",models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4dNkAyPPzq9",
        "outputId": "08b3e456-0284-4688-80d9-76637f269f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 136kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 570kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 238kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [07:37<00:00, 3.10MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 12.3MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:01<00:00, 506kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 319kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model,params)\n",
        "model.to(device)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "iFXX7OdcPssp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "9nG3Qfk-Ecz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "V4pOGtkuEZS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#here we doesn't train we just passing the input to the model and will see what input it gives us"
      ],
      "metadata": {
        "id": "zKSWnBG0bDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(test_data[1])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njvtWtkBULa9",
        "outputId": "9f0ebba3-97eb-4ff9-bd1a-0276c3125012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "## Instruction:\n",
            "Reword the following sentence to the future tense.\n",
            "\n",
            "## Input:\n",
            "He is reading a novel inspired by his grandmother.\n",
            "\n",
            "## Response:\n",
            "He will be reading a novel inspired by his grandmother.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ],
      "metadata": {
        "id": "ifcsRlMrUOWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN9kenn3UhM7",
        "outputId": "f96c42e2-53c1-4644-f04b-933040bb6548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Output:\n",
            "\n",
            "He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "## Example:\n",
            "\n",
            "He is reading a novel inspired by his grandmother.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FINETUNING THE LLM ON INSTRUCTION DATA"
      ],
      "metadata": {
        "id": "Al1R9MqIUup8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FPK5SPQhbZ1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Testing loss:\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv6y1nQ8bdlE",
        "outputId": "06bb466a-3730-4b7d-cdbc-ff59a9e5e187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.8503973484039307\n",
            "Testing loss: 3.780237627029419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(gpt, train_loader, test_loader, device, eval_iter):\n",
        "    gpt.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=eval_iter)\n",
        "        test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=eval_iter)\n",
        "    gpt.train()\n",
        "    return train_loss, test_loss"
      ],
      "metadata": {
        "id": "vR1mu5-VcqnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, test_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, test_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                test_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return train_losses, test_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "CsF90LwUbzn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses, test_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, test_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9oz2L9nb519",
        "outputId": "a618e8d4-e04b-45a8-8c26-a852c24c0f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.552, Val loss 2.537\n",
            "Ep 1 (Step 000005): Train loss 1.043, Val loss 1.089\n",
            "Ep 1 (Step 000010): Train loss 0.870, Val loss 0.982\n",
            "Ep 1 (Step 000015): Train loss 0.803, Val loss 0.916\n",
            "Ep 1 (Step 000020): Train loss 0.839, Val loss 0.873\n",
            "Ep 1 (Step 000025): Train loss 0.762, Val loss 0.849\n",
            "Ep 1 (Step 000030): Train loss 0.676, Val loss 0.824\n",
            "Ep 1 (Step 000035): Train loss 0.672, Val loss 0.814\n",
            "Ep 1 (Step 000040): Train loss 0.680, Val loss 0.798\n",
            "Ep 1 (Step 000045): Train loss 0.634, Val loss 0.785\n",
            "Ep 1 (Step 000050): Train loss 0.667, Val loss 0.777\n",
            "Ep 1 (Step 000055): Train loss 0.550, Val loss 0.771\n",
            "Ep 1 (Step 000060): Train loss 0.601, Val loss 0.767\n",
            "Ep 1 (Step 000065): Train loss 0.500, Val loss 0.753\n",
            "Ep 1 (Step 000070): Train loss 0.605, Val loss 0.738\n",
            "Ep 1 (Step 000075): Train loss 0.544, Val loss 0.738\n",
            "Ep 1 (Step 000080): Train loss 0.509, Val loss 0.740\n",
            "Ep 1 (Step 000085): Train loss 0.597, Val loss 0.733\n",
            "Ep 1 (Step 000090): Train loss 0.472, Val loss 0.721\n",
            "Ep 1 (Step 000095): Train loss 0.481, Val loss 0.721\n",
            "Ep 1 (Step 000100): Train loss 0.476, Val loss 0.722\n",
            "Ep 1 (Step 000105): Train loss 0.464, Val loss 0.721\n",
            "Ep 1 (Step 000110): Train loss 0.540, Val loss 0.717\n",
            "Ep 1 (Step 000115): Train loss 0.427, Val loss 0.714\n",
            "Ep 1 (Step 000120): Train loss 0.434, Val loss 0.708\n",
            "Training completed in 1.66 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6YvL6pI2dCHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, test_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "OTUOXfVUdiGF",
        "outputId": "fd2cc1aa-d972-4194-e176-0275f5bcfa51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples = [\n",
        "    {\n",
        "        \"instruction\": \"Name a synonym for \\\"happy.\\\"\",\n",
        "        \"output\": \">> A synonym for \\\"happy\\\" is \\\"joyful.\\\"\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What is the capital of Japan?\",\n",
        "        \"output\": \">> The capital of Japan is Tokyo.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Convert 10 kilometers to meters.\",\n",
        "        \"output\": \">> 10 kilometers is 10,000 meters.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Name a synonym for 'angry.'\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"A synonym for \\\"angry\\\" is \\\"furious.\\\"\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Convert 12 kilometers to meters.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"12 kilometers is 12,000 meters.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What is the capital of France?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The capital of France is Paris.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Reword the following sentence to future tense.\",\n",
        "        \"input\": \"She cooks dinner every night.\",\n",
        "        \"output\": \"She will cook dinner every night.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Define the term 'photosynthesis'.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"Photosynthesis is the process by which green plants make their own food using sunlight, carbon dioxide, and water.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Rewrite the sentence using a simile.\",\n",
        "        \"input\": \"He is very strong.\",\n",
        "        \"output\": \"He is as strong as an ox.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Convert the following sentence to active voice.\",\n",
        "        \"input\": \"The book was read by the teacher.\",\n",
        "        \"output\": \"The teacher read the book.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"What is the past tense of 'swim'?\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The past tense of 'swim' is 'swam'.\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Generate a question based on this statement.\",\n",
        "        \"input\": \"Water boils at 100 degrees Celsius.\",\n",
        "        \"output\": \"What temperature does water boil at?\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Name the largest planet in our solar system.\",\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"The largest planet in our solar system is Jupiter.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "U7glIVo1ltZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input_text(entry):\n",
        "    input_text = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "    input_text += f\"### Instruction:\\n{entry['instruction']}\\n\"\n",
        "    if 'input' in entry and entry['input']:\n",
        "        input_text += f\"\\n### Input:\\n{entry['input']}\\n\"\n",
        "    return input_text\n"
      ],
      "metadata": {
        "id": "zjoIw7-Bma8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Loop through examples\n",
        "for entry in test_examples:\n",
        "    input_text = format_input_text(entry)\n",
        "\n",
        "    # Generate tokens\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "\n",
        "    # Decode text\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    # Extract model response only\n",
        "    response_text = generated_text[len(input_text):].strip()\n",
        "    for tag in [\"### Response:\", \"## Response:\", \"Response:\", \"### Output:\", \"## Output:\", \"Output:\"]:\n",
        "        if tag in response_text:\n",
        "            response_text = response_text.split(tag, 1)[-1].strip()\n",
        "\n",
        "    # Print results\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n{entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0XVufiVdv6e",
        "outputId": "832544e3-918e-48be-9ee3-4887bbb2feed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name a synonym for \"happy.\"\n",
            "\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for \"happy\" is \"joyful.\"\n",
            "\n",
            "Model response:\n",
            ">> A synonym for \"happy\" is \"joyful.\"\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of Japan?\n",
            "\n",
            "\n",
            "Correct response:\n",
            ">> The capital of Japan is Tokyo.\n",
            "\n",
            "Model response:\n",
            ">> The capital of Japan is Tokyo.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 10 kilometers to meters.\n",
            "\n",
            "\n",
            "Correct response:\n",
            ">> 10 kilometers is 10,000 meters.\n",
            "\n",
            "Model response:\n",
            ">> 10 kilometers is 1000 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name a synonym for 'angry.'\n",
            "\n",
            "\n",
            "Correct response:\n",
            "A synonym for \"angry\" is \"furious.\"\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'angry' is 'angry.'\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 12 kilometers to meters.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "12 kilometers is 12,000 meters.\n",
            "\n",
            "Model response:\n",
            ">> 12 kilometers is 12 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of France?\n",
            "\n",
            "\n",
            "Correct response:\n",
            "The capital of France is Paris.\n",
            "\n",
            "Model response:\n",
            ">> The capital of France is Paris.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Reword the following sentence to future tense.\n",
            "\n",
            "### Input:\n",
            "She cooks dinner every night.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "She will cook dinner every night.\n",
            "\n",
            "Model response:\n",
            ">> She cooks dinner every night.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Define the term 'photosynthesis'.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "Photosynthesis is the process by which green plants make their own food using sunlight, carbon dioxide, and water.\n",
            "\n",
            "Model response:\n",
            ">> Photosynthesis is the process by which plants use sunlight to synthesize food and energy from carbon dioxide and water.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "He is very strong.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "He is as strong as an ox.\n",
            "\n",
            "Model response:\n",
            ">> He is as strong as a bull.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the following sentence to active voice.\n",
            "\n",
            "### Input:\n",
            "The book was read by the teacher.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "The teacher read the book.\n",
            "\n",
            "Model response:\n",
            ">> The teacher read the book.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the past tense of 'swim'?\n",
            "\n",
            "\n",
            "Correct response:\n",
            "The past tense of 'swim' is 'swam'.\n",
            "\n",
            "Model response:\n",
            ">> The past tense of 'swim' is 'swam'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a question based on this statement.\n",
            "\n",
            "### Input:\n",
            "Water boils at 100 degrees Celsius.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "What temperature does water boil at?\n",
            "\n",
            "Model response:\n",
            ">> Water boils at 100 degrees Celsius.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the largest planet in our solar system.\n",
            "\n",
            "\n",
            "Correct response:\n",
            "The largest planet in our solar system is Jupiter.\n",
            "\n",
            "Model response:\n",
            ">> The largest planet in our solar system is Pluto.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#As we can see based on the test set instructions, given responses, and the model's responses, the model performs relatively well."
      ],
      "metadata": {
        "id": "7pp7be59lOzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The answers for few question is clearly correct and some are not correct**"
      ],
      "metadata": {
        "id": "um9q6pFVnWXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# we save the model as gpt2-medium355M-instruction-finetuned.pth file to be able to reuse it in future projects:"
      ],
      "metadata": {
        "id": "gUkeBmXwsvQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"gpt2-medium355M-instruction-finetuned.pth\")\n",
        "print(f\"Model saved as gpt2-medium355M-instruction-finetuned.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QseD7_SoVtp",
        "outputId": "a1542e14-a385-41a3-9b76-63c580b6e4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-instruction-finetuned.pth\n"
          ]
        }
      ]
    }
  ]
}